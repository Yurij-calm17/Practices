import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LeakyReLU
from tensorflow.keras.utils import to_categorical

# 1. –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö (—è–∫ —É –ó–∞–≤–¥–∞–Ω–Ω—ñ 2)
np.random.seed(42)
n_samples = 1000
X = np.random.rand(n_samples, 4) * 100
y = []
for row in X:
    if row[0] > 70 and row[1] > 60:
        y.append(1)
    elif row[2] > 50:
        y.append(2)
    else:
        y.append(0)
y = np.array(y)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)

y_train_cat = to_categorical(y_train, num_classes=3)
y_test_cat = to_categorical(y_test, num_classes=3)

# 2. –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –ø–æ–±—É–¥–æ–≤–∏ —ñ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π
def build_and_train_model(activation_name):
    model = Sequential()
    if activation_name == 'leaky_relu':
        model.add(Dense(32, input_shape=(4,)))
        model.add(LeakyReLU(alpha=0.1))
        model.add(Dense(16))
        model.add(LeakyReLU(alpha=0.1))
    else:
        model.add(Dense(32, activation=activation_name, input_shape=(4,)))
        model.add(Dense(16, activation=activation_name))
    model.add(Dense(3, activation='softmax'))

    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    history = model.fit(X_train, y_train_cat, epochs=50, batch_size=16, verbose=0, validation_split=0.2)
    return history

# 3. –ü–µ—Ä–µ–ª—ñ–∫ —Ñ—É–Ω–∫—Ü—ñ–π –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó
activations = ['relu', 'sigmoid', 'tanh', 'leaky_relu']
histories = {}

for act in activations:
    print(f"üîÅ –ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –∑ –∞–∫—Ç–∏–≤–∞—Ü—ñ—î—é: {act}")
    histories[act] = build_and_train_model(act)

# 4. –ü–æ–±—É–¥–æ–≤–∞ –≥—Ä–∞—Ñ—ñ–∫—ñ–≤
plt.figure(figsize=(14, 6))

for i, act in enumerate(activations):
    plt.subplot(1, 2, 1)
    plt.plot(histories[act].history['val_accuracy'], label=act)
    plt.title("Validation Accuracy")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(histories[act].history['val_loss'], label=act)
    plt.title("Validation Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()

plt.tight_layout()
plt.show()

#–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Ñ—É–Ω–∫—Ü—ñ–π –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó –ø–æ–∫–∞–∑–∞–ª–æ, —â–æ –Ω–∞–π–∫—Ä–∞—â—É —Ç–æ—á–Ω—ñ—Å—Ç—å —Ç–∞ —à–≤–∏–¥–∫—ñ—Å—Ç—å –∑–±—ñ–∂–Ω–æ—Å—Ç—ñ –¥–µ–º–æ–Ω—Å—Ç—Ä—É—î ReLU.
–§—É–Ω–∫—Ü—ñ—ó Sigmoid —Ç–∞ Tanh –º–∞—é—Ç—å –ø–æ–≤—ñ–ª—å–Ω—ñ—à—É –¥–∏–Ω–∞–º—ñ–∫—É –Ω–∞–≤—á–∞–Ω–Ω—è.
LeakyReLU –ø–æ–∫–∞–∑–∞–ª–∞ —Å—Ç–∞–±—ñ–ª—å–Ω—É —Ä–æ–±–æ—Ç—É, –¥–µ–º–æ–Ω—Å—Ç—Ä—É—é—á–∏ –≥–∞—Ä–Ω—É –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å –∑–∞–≤–¥—è–∫–∏ —É–Ω–∏–∫–Ω–µ–Ω–Ω—é –ø—Ä–æ–±–ª–µ–º–∏ –Ω—É–ª—å–æ–≤–∏—Ö –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤.